"""
# íŒŒì¼ëª… : relation_processor.py
# ì„¤ëª…   : ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ processor
# ê°œì •ì´ë ¥ :
#    ë²„ì ¼    ìˆ˜ì •ì¼ì                   ìˆ˜ì •ì              ë‚´ìš©
#    1.0    2022-05-13                  ê¹€ì¢…ì™„            ì‹ ê·œ ì‘ì„±
""" 
from sklearn.utils import column_or_1d
from collections import OrderedDict
from abc import ABC, abstractmethod
from transformers import PreTrainedTokenizer
import pandas as pd
from pandas import DataFrame
from typing import Iterable, Tuple, List
import os
from tqdm.auto import tqdm
import json
import re
#common
from common import Paths
from common import Params
from common import createFolder
#logging setting
from common import Logging_config
logging_instance = Logging_config(Params.CUSTOM_LOG_FILE, Paths.LOG_DIR)
logger = logging_instance.logging_setting()



"""
# Class  : DataParing
# ì„¤ëª…   : 
"""
class DataParsing :
    TSV_NAMES       = ["relation", "cause", "result", "sentence"]
    DF_TO_JSON_DICT = {
        "train" : "",
        "val"   : "",
        "test"  : "",
    }

    @staticmethod
    def search_token(series_data):
        regex = re.compile(r"\<\/*e\d\>")
        find_token_seq = regex.findall(series_data[DataParsing.TSV_NAMES[3]]) # "sentence"

        return find_token_seq


"""
# Class  : TrainParams
# ì„¤ëª…   : trainì‹œì— í•„ìš”í•œ parameters ì •ì˜
"""
class TrainParams :
    # Data Mapping
    DATASET_MAPPING = {
        "CausalData": {
            "dir": Paths.CAUSAL_DATA_DIR,
            "keep_test_order": True,
            "precision_recall_curve_baseline_img": None,
        },
    }

    SUB_OBJ_DICT = {
        "SUB_CHAR" : ["[", "]"],
        "OBJ_CHAR" : ["{", "}"],
    }

    # BERT variants
    AVAILABLE_PRETRAINED_MODELS = [
        "bert-base-uncased",       
        "roberta-base",            
        "bert-large-uncased",      
        "bert-large-uncased",      
        "bert-large-uncased",      
        "beomi/kcbert-base",      
    ]


"""
# Class  : OrdinalLabelEncoder
# ì„¤ëª…   : # TODO OrdinalLabelEncoder ì„¤ëª…
"""
class OrdinalLabelEncoder:
    def __init__(self, init_labels=None):
        if init_labels is None:
            init_labels = []
        self.mapping = OrderedDict({l: i for i, l in enumerate(init_labels)})

    @property
    def classes_(self):
        return list(self.mapping.keys())

    def fit_transform(self, y):
        return self.fit(y).transform(y) # TODO ì—¬ê¸°ì„œ label ë²ˆí˜¸ë¡œ í† í° ë§¤í•‘í•´ì¤€ ë¦¬ìŠ¤íŠ¸ return

    def fit(self, y):
        y = column_or_1d(y, warn=True)
        new_classes = pd.Series(y).unique()
        # TODO ìœ„ì— otherë¡œ mapping ë§Œë“¤ì–´ì¤€ê±°ì— relation ì¶”ê°€
        for cls in new_classes:
            if cls not in self.mapping:
                self.mapping[cls] = len(self.mapping)
        return self

    def transform(self, y):
        y = column_or_1d(y, warn=True)
        return [self.mapping[value] for value in y]



"""
# Class  : AbstractPreprocessor
# ì„¤ëª…   : # TODO AbstractPreprocessor ì„¤ëª…
"""
class AbstractPreprocessor(ABC, Paths, TrainParams):
    DATASET_NAME        = ""   # TODO ì´ë ‡ê²Œ ê¼­ ì§€ì •í•´ì„œ í•´ì•¼ í•˜ë‚˜ ì‹¶ë‹¤.
    VAL_DATA_PROPORTION = 0.2
    NO_RELATION_LABEL   = ""

    def __init__(self, tokenizer: PreTrainedTokenizer):
        self.tokenizer = tokenizer
        # TODO ìŠ¤í˜ì…œ í† í° ê³ ë ¤?? 
        # TODO ìŠ¤í˜ì…œ í† í° ì¶”ê°€ 
        self.SUB_START_ID, self.SUB_END_ID, self.OBJ_START_ID, self.OBJ_END_ID \
            = tokenizer.convert_tokens_to_ids([TrainParams.SUB_OBJ_DICT['SUB_CHAR'][0], TrainParams.SUB_OBJ_DICT['SUB_CHAR'][1], # start, end
                                               TrainParams.SUB_OBJ_DICT['OBJ_CHAR'][0], TrainParams.SUB_OBJ_DICT['OBJ_CHAR'][1]] # start, end
                                               )
        self.label_encoder   = OrdinalLabelEncoder([self.NO_RELATION_LABEL]) # TODO ì´ê±° ë¹¼ë„ ë˜ì§€ ì•Šë‚˜ ì‹¶ìŒ
        self.df_to_json_dict = DataParsing.DF_TO_JSON_DICT

    # TODO 1.
    def preprocess_data(self):
        logger.info(f"\n---> Preprocessing {self.DATASET_NAME} dataset <---")

        # í´ë”ê°€ ì—†ë‹¤ë©´ í´ë” ìƒì„±
        createFolder(self.PROCESSED_DATA_DIR)
        train_data, val_data, test_data = self._preprocess_data() # go to _02_causal_data_integrate_preprocess_and_split.py  _preprocess_data

        # TODO ì´ë¦„ ë°”ê¿”ì•¼í•¨.
        logger.info("Create to json format String")
        data_json_tp = self._write_data_to_file([train_data, val_data, test_data]) # return tuple format data

        # TODO ë©”íƒ€ë°ì´í„° ë§Œë“œëŠ” ë¶€ë¶„
        self._save_metadata({
            "train_size": len(train_data),
            "val_size"  : len(val_data),
            "test_size" : len(test_data),
            "no_relation_label": self.NO_RELATION_LABEL,
            **self._get_label_mapping() # TODO _get_label_mapping
        })

        self._create_secondary_data_files(data_json_tp) # create relation json 
        logger.info("---> Done ! <---")


    def _create_secondary_data_files(self, data_json_tp:Tuple[str,str,str]):
        """
        From the primary data file, create a data file with binary labels
        and a data file with only sentences classified as "related"
        """
        # read metadata.json
        with open(self.METADATA_FILE_NAME, encoding='utf-8') as f:
            root_metadata  = json.load(f)
            metadata       = root_metadata[self.DATASET_NAME]

        related_only_count = {
            "train": 0,
            "val": 0,
            "test": 0,
        }
        related_only_count_keys_list = list(related_only_count.keys())

        for key, str_json in zip(related_only_count_keys_list, data_json_tp) :
            logger.info(f"Creating relation files for {key} data")
            total = metadata[f"{key}_size"]
            str_json_list = str_json.split("\n")[:-1] # "\n"ìœ¼ë¡œ splitì‹œì— ë§ˆì§€ë§‰ì´ ë¹ˆê°’ì„

            # relation data json save
            with open(self.TRAIN_VAL_TEST_PATH[key], "w", encoding='utf-8') as save_json :
                for line in tqdm(str_json_list, total=total) : 
                    data = json.loads(line)      # read str of dict format
                    related_only_count[key] += 1 # data count update
                    
                    # TODO ğŸ’¡ì´ë¶€ë¶„ otherê°€ ë¹ ì§€ë©´ ì—†ì–´ë„ ë ë“¯?
                    data["label"] -= 1 # label in "related_only" files is 1 less than the original label

                    save_json.write(json.dumps(data, ensure_ascii=False) + "\n")
            
            # relation count update in json            
            metadata[f"{key}_related_only_size"] = related_only_count[key]
        
        # change data in json
        logger.info("Updating metadata.json")
        root_metadata[self.DATASET_NAME] = metadata
        with open(self.METADATA_FILE_NAME, "w", encoding='utf-8') as f:
            json.dump(root_metadata, f, indent=4, ensure_ascii=False)


    def _write_data_to_file(self, df_data_list: List[DataFrame]) -> Tuple[str, str, str]:
        """ìˆœì„œ : train/val/test"""
        key_list = list(self.df_to_json_dict)
        for df, key in zip(df_data_list, key_list) :
            lines = ""
            for _, row in df.iterrows():
                lines += row.to_json() + "\n"

            self.df_to_json_dict[key] = lines

        return self.df_to_json_dict[key_list[0]], self.df_to_json_dict[key_list[1]], self.df_to_json_dict[key_list[2]]


    def _find_sub_obj_pos(self, input_ids_list: Iterable) -> DataFrame:
        """
        Find subject and object position in a sentence
        """
        sub_start_pos = [self._index(s, self.SUB_START_ID) + 1 for s in input_ids_list]
        sub_end_pos   = [self._index(s, self.SUB_END_ID, sub_start_pos[i]) for i, s in enumerate(input_ids_list)]
        obj_start_pos = [self._index(s, self.OBJ_START_ID) + 1 for s in input_ids_list]
        obj_end_pos   = [self._index(s, self.OBJ_END_ID, obj_start_pos[i]) for i, s in enumerate(input_ids_list)]
        return DataFrame({
            "sub_start_pos": sub_start_pos,
            "sub_end_pos"  : sub_end_pos,
            "obj_start_pos": obj_start_pos,
            "obj_end_pos"  : obj_end_pos,
        })

    @staticmethod
    def _index(lst: list, ele: int, start: int = 0) -> int:
        """
        Find an element in a list. Returns -1 if not found instead of raising an exception.
        """
        try:
            return lst.index(ele, start)
        except ValueError:
            return -1

    # TODO _clean_data ì—¬ê¸°ì„œ ë°ì´í„° ì •í•©ì„±ìœ¼ë¡œ kcbertëŠ” maxê°€ 300 ì´ë¯€ë¡œ ì´ˆê³¼ëŠ” ë»„
    def _clean_data(self, raw_sentences: list, labels: list) -> DataFrame:
        if not raw_sentences:
            return DataFrame()
        # TODO ì¤‘ìš”!!! í† í°í™”!!!
        tokens = self.tokenizer(raw_sentences, truncation=True, padding="max_length")

        data   = DataFrame(tokens.data)
        data["label"]    = self.label_encoder.fit_transform(labels)
        sub_obj_position = self._find_sub_obj_pos(data["input_ids"])
        data   = pd.concat([data, sub_obj_position], axis=1)
        data   = self._remove_invalid_sentences(data)
        return data

    def _remove_invalid_sentences(self, data: DataFrame) -> DataFrame:
        """
        Remove sentences without subject/object or whose subject/object
        is beyond the maximum length the model supports
        """
        # TODO max length ë¶€ë¶„ ì§€ê¸ˆ 05_27 15:04 300ìœ¼ë¡œ ì„¸íŒ…ë˜ì–´ ìˆìŒ
        seq_max_len = self.tokenizer.model_max_length
        return data.loc[
              (data["sub_end_pos"] < seq_max_len)
            & (data["obj_end_pos"] < seq_max_len)
            & (data["sub_end_pos"] > -1)
            & (data["obj_end_pos"] > -1)
        ]
    # TODO _get_label_mapping ë©”ì†Œë“œ
    def _get_label_mapping(self):
        """
        Returns a mapping from id to label and vise versa from the label encoder
        """
        # all labels
        id_to_label = dict(enumerate(self.label_encoder.classes_))
        label_to_id = {v: k for k, v in id_to_label.items()}

        # for the related_only dataset
        # ignore id 0, which represent no relation

        # TODO ë”•ì…”ë„ˆë¦¬ ë¼ë²¨ê³¼ idì™€ key value êµí™˜ì‹
        id_to_label_related_only = {k - 1: v for k, v in id_to_label.items() if k != 0}
        label_to_id_related_only = {v: k for k, v in id_to_label_related_only.items()}

        return {
            "id_to_label": id_to_label,
            "label_to_id": label_to_id,
            "id_to_label_related_only": id_to_label_related_only,
            "label_to_id_related_only": label_to_id_related_only,            
        }

    # TODO _save_metadata ë©”ì†Œë“œ
    def _save_metadata(self, metadata: dict):
        """Save metadata to metadata.json"""
        # create metadata file
        if not os.path.exists(self.METADATA_FILE_NAME):
            logger.info(f"Create metadata file at {self.METADATA_FILE_NAME}")
            with open(self.METADATA_FILE_NAME, "w", encoding='utf-8') as f:
                f.write("{}\n")

        # add metadata
        logger.info("Saving metadata")
        with open(self.METADATA_FILE_NAME, encoding="utf-8") as f:
            root_metadata = json.load(f)
        with open(self.METADATA_FILE_NAME, "w", encoding='utf-8') as f:
            root_metadata[self.DATASET_NAME] = metadata
            json.dump(root_metadata, f, indent=4, ensure_ascii=False)
    
    @classmethod
    def get_dataset_file_name(cls, key: str) -> str:
        return os.path.join(Paths.PROCESSED_DATA_DIR, f"{cls.DATASET_NAME.lower()}_{key}.json")