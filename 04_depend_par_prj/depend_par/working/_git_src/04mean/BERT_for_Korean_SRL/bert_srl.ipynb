{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from BERT_for_Korean_SRL import dataio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import glob\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = 6\n",
    "\n",
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn: 19306\n",
      "tst: 3778\n",
      "data example\n",
      "[['한국탁구가', '2000년', '시드니올림픽', '본선에', '남녀복식', '2개조씩을', '<tgt>', '파견할', '</tgt>', '수', '있게', '됐다.'], ['_', '_', '_', '_', '_', '_', '_', '파견.01', '_', '_', '_', '_'], ['ARG0', 'O', 'O', 'ARG2', 'O', 'ARG1', 'X', 'O', 'X', 'O', 'AUX', 'AUX']]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    trn, tst = dataio.load_srl_data_for_bert()\n",
    "    print('trn:', len(trn))\n",
    "    print('tst:', len(tst))\n",
    "    print('data example')\n",
    "    print(trn[0])\n",
    "    \n",
    "    return trn, tst\n",
    "\n",
    "# trn, tst = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class for_BERT():\n",
    "    \n",
    "    def __init__(self, mode='training'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        with open(dir_path+'/data/tag2idx.json','r') as f:\n",
    "            self.tag2idx = json.load(f)\n",
    "            \n",
    "        self.idx2tag = dict(zip(self.tag2idx.values(),self.tag2idx.keys()))\n",
    "        \n",
    "        # load pretrained BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "        \n",
    "        # load BERT tokenizer with untokenizing frames\n",
    "        never_split_tuple = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    "        added_never_split = []\n",
    "        added_never_split.append('<tgt>')\n",
    "        added_never_split.append('</tgt>')\n",
    "        added_never_split_tuple = tuple(added_never_split)\n",
    "        never_split_tuple += added_never_split_tuple\n",
    "        vocab_file_path = dir_path+'/data/bert-multilingual-cased-dict-add-frames'\n",
    "        self.tokenizer_with_frame = BertTokenizer(vocab_file_path, do_lower_case=False, max_len=256, never_split=never_split_tuple)\n",
    "        \n",
    "    def idx2tag(self, predictions):\n",
    "        pred_tags = [self.idx2tag[p_i] for p in predictions for p_i in p]\n",
    "        \n",
    "        # bert tokenizer and assign to the first token\n",
    "    def bert_tokenizer(self, text):\n",
    "        orig_tokens = text.split(' ')\n",
    "        bert_tokens = []\n",
    "        orig_to_tok_map = []\n",
    "        bert_tokens.append(\"[CLS]\")\n",
    "        for orig_token in orig_tokens:\n",
    "            orig_to_tok_map.append(len(bert_tokens))\n",
    "            bert_tokens.extend(self.tokenizer_with_frame.tokenize(orig_token))\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "\n",
    "        return orig_tokens, bert_tokens, orig_to_tok_map\n",
    "    \n",
    "    def convert_to_bert_input(self, input_data):\n",
    "        tokenized_texts, args = [],[]\n",
    "        orig_tok_to_maps = []\n",
    "        for i in range(len(input_data)):    \n",
    "            data = input_data[i]\n",
    "            text = ' '.join(data[0])\n",
    "            orig_tokens, bert_tokens, orig_to_tok_map = self.bert_tokenizer(text)\n",
    "            orig_tok_to_maps.append(orig_to_tok_map)\n",
    "            tokenized_texts.append(bert_tokens)\n",
    "\n",
    "            if self.mode == 'training':\n",
    "                ori_args = data[2]\n",
    "                arg_sequence = []\n",
    "                for i in range(len(bert_tokens)):\n",
    "                    if i in orig_to_tok_map:\n",
    "                        idx = orig_to_tok_map.index(i)\n",
    "                        ar = ori_args[idx]\n",
    "                        arg_sequence.append(ar)\n",
    "                    else:\n",
    "                        arg_sequence.append('X')\n",
    "                args.append(arg_sequence)\n",
    "\n",
    "        input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        orig_tok_to_maps = pad_sequences(orig_tok_to_maps, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=-1)\n",
    "        \n",
    "        if self.mode =='training':\n",
    "            arg_ids = pad_sequences([[self.tag2idx.get(ar) for ar in arg] for arg in args],\n",
    "                                    maxlen=MAX_LEN, value=self.tag2idx[\"X\"], padding=\"post\",\n",
    "                                    dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "        attention_masks = [[float(i>0) for i in ii] for ii in input_ids]    \n",
    "        data_inputs = torch.tensor(input_ids)\n",
    "        data_orig_tok_to_maps = torch.tensor(orig_tok_to_maps)\n",
    "        data_masks = torch.tensor(attention_masks)\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            data_args = torch.tensor(arg_ids)\n",
    "            bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_args, data_masks)\n",
    "        else:\n",
    "            bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_masks)\n",
    "        return bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_io = for_BERT(mode='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_data = bert_io.convert_to_bert_input(trn)\n",
    "# tst_data = bert_io.convert_to_bert_input(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model_path = '/disk/data/models/kosrl_1105/'\n",
    "    print('your model would be saved at', model_path)\n",
    "    \n",
    "    model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(bert_io.tag2idx))\n",
    "    model.to(device);\n",
    "    \n",
    "    trn_data = bert_io.convert_to_bert_input(trn)\n",
    "    sampler = RandomSampler(trn_data)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    \n",
    "    # train \n",
    "    epochs = 10\n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_args, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_masks, labels=b_input_args)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "#             break\n",
    "#         break\n",
    "\n",
    "        # print train loss per epoch\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        model_saved_path = model_path+'ko-srl-epoch-'+str(num_of_epoch)+'.pt'        \n",
    "        torch.save(model, model_saved_path)\n",
    "        num_of_epoch += 1\n",
    "    print('...training is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your model would be saved at ./models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.515737771987915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 1/10 [00:00<00:07,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.9902143478393555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 2/10 [00:01<00:06,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.462798595428467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 3/10 [00:02<00:05,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.855548620223999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 4/10 [00:03<00:04,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.5079286098480225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 5/10 [00:03<00:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1641660928726196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 6/10 [00:04<00:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.010244369506836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 7/10 [00:05<00:02,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9694228768348694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 8/10 [00:06<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.995982825756073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 9/10 [00:06<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8896213173866272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def test():\n",
    "    model_path = model_path = '/disk/data/models/kosrl_1105/'\n",
    "    models = glob.glob(model_path+'*.pt')\n",
    "    \n",
    "    result_path = model_path = '/disk/data/models/result_kosrl_1105/'\n",
    "    results = []\n",
    "    \n",
    "    for m in models:\n",
    "        print('model:', m)\n",
    "        model = torch.load(m)\n",
    "        model.eval()\n",
    "        \n",
    "        tst_data = bert_io.convert_to_bert_input(tst)\n",
    "        sampler = RandomSampler(tst_data)\n",
    "        tst_dataloader = DataLoader(tst_data, sampler=sampler, batch_size=batch_size)\n",
    "        \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        \n",
    "        pred_args, true_args = [],[]\n",
    "        for batch in tst_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_args, b_input_masks = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_masks, labels=b_input_args)\n",
    "                logits = model(b_input_ids, token_type_ids=None,\n",
    "                               attention_mask=b_input_masks)\n",
    "                \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            \n",
    "            b_pred_args = [list(p) for p in np.argmax(logits, axis=2)]\n",
    "            b_true_args = b_input_args.to('cpu').numpy().tolist()\n",
    "            \n",
    "            \n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            \n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            for b_idx in range(len(b_true_args)):\n",
    "                \n",
    "                input_id = b_input_ids[b_idx]\n",
    "                orig_tok_to_map = b_input_orig_tok_to_maps[b_idx]                \n",
    "                pred_arg_bert = b_pred_args[b_idx]\n",
    "                true_arg_bert = b_true_args[b_idx]\n",
    "\n",
    "                pred_arg, true_arg = [],[]\n",
    "                for tok_idx in orig_tok_to_map:\n",
    "                    if tok_idx != -1:\n",
    "                        tok_id = int(input_id[tok_idx])\n",
    "                        if tok_id == 1:\n",
    "                            pass\n",
    "                        elif tok_id == 2:\n",
    "                            pass\n",
    "                        else:\n",
    "                            pred_arg.append(pred_arg_bert[tok_idx])\n",
    "                            true_arg.append(true_arg_bert[tok_idx])\n",
    "                            \n",
    "                pred_args.append(pred_arg)\n",
    "                true_args.append(true_arg) \n",
    "            \n",
    "#             break\n",
    "\n",
    "        \n",
    "        pred_arg_tags_old = [[bert_io.idx2tag[p_i] for p_i in p] for p in pred_args]\n",
    "        \n",
    "        pred_arg_tags = []\n",
    "        for old in pred_arg_tags_old:\n",
    "            new = []\n",
    "            for t in old:\n",
    "                if t == 'X':\n",
    "                    new_t = 'O'\n",
    "                else:\n",
    "                    new_t = t\n",
    "                new.append(new_t)\n",
    "            pred_arg_tags.append(new)\n",
    "            \n",
    "        valid_arg_tags = [[bert_io.idx2tag[v_i] for v_i in v] for v in true_args]\n",
    "        f1 = f1_score(pred_arg_tags, valid_arg_tags)\n",
    "                \n",
    "        print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        print(\"Validation F1-Score: {}\".format(f1_score(pred_arg_tags, valid_arg_tags)))\n",
    "                \n",
    "        result =  m+'\\targid:'+str(f1)+'\\n'\n",
    "        results.append(result)\n",
    "        \n",
    "        epoch = m.split('-')[-1].split('.')[0]\n",
    "        fname = result_path+str(epoch)+'-result.txt'\n",
    "        \n",
    "        with open(fname, 'w') as f:\n",
    "            line = result\n",
    "            f.write(line)\n",
    "            line = 'gold'+'\\t'+'pred'+'\\n'\n",
    "            f.write(line)\n",
    "            \n",
    "            for r in range(len(pred_arg_tags)):\n",
    "                line = str(valid_arg_tags[r]) + '\\t' + str(pred_arg_tags[r])+'\\n'\n",
    "                f.write(line)\n",
    "                \n",
    "    fname = result_path+'result.txt'\n",
    "    with open(fname, 'w') as f:\n",
    "        for r in results:\n",
    "            f.write(r)\n",
    "            \n",
    "    print('result is written to',fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ./models/ko-srl-epoch-6.pt\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ARG1', 'O', 'O', 'ARG0', 'O', 'O', 'O', 'O'], ['O', 'O', 'ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ARG1', 'O', 'AUX', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ARG1', 'O', 'AUX'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'ARGM-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ARG1', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ARG2', 'O', 'ARG1', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'ARGM-TMP', 'O', 'O', 'O', 'ARG2', 'O', 'O', 'O', 'O', 'ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-a2bda5a8f818>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_arg_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m#         pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_tags' is not defined"
     ]
    }
   ],
   "source": [
    "# test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
